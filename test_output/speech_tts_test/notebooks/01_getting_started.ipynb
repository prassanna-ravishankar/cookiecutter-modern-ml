{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech TTS Test - Getting Started\n",
    "\n",
    "This notebook demonstrates the complete workflow for fine-tuning a model using our modern ML template.\n",
    "\n",
    "## üöÄ What we'll cover:\n",
    "1. Environment setup and device detection\n",
    "2. Data loading and analysis with Polars\n",
    "3. Model training with Mac MPS support\n",
    "4. Model serving and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport logging\nfrom pathlib import Path\n\n# Add package to path for imports\nsys.path.append(str(Path.cwd().parent))\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nprint(\"‚úÖ Environment setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom speech_tts_test.config import get_settings\nfrom speech_tts_test.models.train_model import get_device\n\n# Load configuration\nsettings = get_settings()\nprint(f\"üìä Model: {settings.model.checkpoint}\")\nprint(f\"üìö Dataset: {settings.training.dataset_name}\")\nprint(f\"‚öôÔ∏è Epochs: {settings.training.num_train_epochs}\")\n\n# Detect device\ndevice = get_device()\nprint(f\"üîß Using device: {device}\")\n\n# Show device info\nif device == \"cuda\":\n    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelif device == \"mps\":\n    print(\"   Mac Metal Performance Shaders enabled üçé\")\nelse:\n    print(\"   CPU training (slower but works everywhere)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from speech_tts_test.data_utils import load_and_process_dataset, analyze_dataset_stats, create_data_summary_report\n\n# Load dataset with smaller subset for notebook demo\nprint(\"üì• Loading dataset...\")\ndataset = load_and_process_dataset(\n    settings.training.dataset_name,\n    subset_size=1000  # Small subset for demo\n)\n\nprint(f\"‚úÖ Loaded {len(dataset)} splits\")\nfor split_name, split_data in dataset.items():\n    print(f\"   {split_name}: {len(split_data)} examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset statistics\n",
    "print(\"üìä Analyzing dataset statistics...\")\n",
    "stats = analyze_dataset_stats(dataset)\n",
    "\n",
    "# Create a detailed report\n",
    "report = create_data_summary_report(dataset)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some examples\n",
    "print(\"üìù Sample data:\")\n",
    "for i in range(3):\n",
    "    example = dataset[\"train\"][i]\n",
    "    sentiment = \"positive\" if example[\"label\"] == 1 else \"negative\"\n",
    "    text_preview = example[\"text\"][:100] + \"...\" if len(example[\"text\"]) > 100 else example[\"text\"]\n",
    "    print(f\"\\n{i+1}. [{sentiment.upper()}] {text_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Now let's train our model. This will work on Mac MPS, CUDA GPUs, or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)\nfrom speech_tts_test.models.train_model import preprocess_function, compute_metrics\n\n# Set seed\nset_seed(42)\n\nprint(\"ü§ó Loading model and tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(settings.model.checkpoint)\n\n# Add padding token if not present\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    settings.model.checkpoint, \n    num_labels=2,\n    torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n)\n\nif tokenizer.pad_token is not None:\n    model.config.pad_token_id = tokenizer.pad_token_id\n\nprint(\"‚úÖ Model loaded successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "print(\"üî§ Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer, settings.model.max_length),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete\")\n",
    "print(f\"Train examples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Test examples: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Adjust batch size for device\n",
    "batch_size = 4 if device in [\"mps\", \"cpu\"] else 8\n",
    "print(f\"üéØ Using batch size: {batch_size}\")\n",
    "\n",
    "output_dir = Path(\"../models\") / settings.model.checkpoint.replace(\"/\", \"-\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,  # Quick training for demo\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False if device == \"mps\" else True,\n",
    "    fp16=False if device in [\"mps\", \"cpu\"] else True,\n",
    "    auto_find_batch_size=True,\n",
    "    report_to=[],  # Disable wandb/tensorboard for notebook\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Training examples: {len(tokenized_datasets['train'])}\")\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"Final training loss: {train_result.training_loss:.4f}\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"This might be due to memory limitations. Try reducing batch size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"üìä Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìà Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inference\n",
    "\n",
    "Let's test our trained model with some examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with custom examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! Great acting and amazing story.\",\n",
    "    \"Terrible film. Waste of time and money. Very disappointed.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"One of the best movies I've ever seen! Highly recommend!\",\n",
    "    \"Boring and predictable. Fell asleep halfway through.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model inference...\\n\")\n",
    "\n",
    "model.eval()\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=settings.model.max_length)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(prediction, dim=-1).item()\n",
    "        confidence = prediction[0][predicted_class].item()\n",
    "    \n",
    "    sentiment = \"üòä POSITIVE\" if predicted_class == 1 else \"üòû NEGATIVE\"\n",
    "    print(f\"{i}. {sentiment} (confidence: {confidence:.3f})\")\n",
    "    print(f\"   Text: \\\"{text}\\\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Model\n",
    "\n",
    "Let's save our trained model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"üíæ Saving model...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {output_dir}\")\n",
    "print(f\"   Model files: {list(output_dir.glob('*'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéâ Success!\n\nYou've successfully:\n1. ‚úÖ Set up the environment with device detection\n2. ‚úÖ Loaded and analyzed data using Polars\n3. ‚úÖ Fine-tuned a model with Mac MPS support\n4. ‚úÖ Evaluated the model performance\n5. ‚úÖ Tested inference with custom examples\n6. ‚úÖ Saved the trained model\n\n## Next Steps:\n\n- **Deploy the model**: Use `uv run task serve` to start the API server\n- **Cloud training**: Use `uv run task train-cloud` for larger datasets (if configured)\n- **Experiment**: Try different models in `configs/settings.yaml`\n- **Scale up**: Remove the `subset_size` parameter for full dataset training\n\nHappy machine learning! üöÄ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}