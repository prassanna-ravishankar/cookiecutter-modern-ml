# {{ cookiecutter.project_name }} Configuration
# ML-centric configuration with modality-aware settings

modality: "{{ cookiecutter.modality }}"

experiment:
  name: "{{ cookiecutter.project_slug }}_experiment"
  seed: 42
  mixed_precision: true
  gradient_checkpointing: true

data:
  {% if cookiecutter.modality == 'speech' -%}
  name: "{{ cookiecutter.dataset_name['speech_' + cookiecutter.speech_task] }}"
  {% else -%}
  name: "{{ cookiecutter.dataset_name[cookiecutter.modality] }}"
  {% endif -%}
  train_split: "train"
  eval_split: "test"
  max_samples: null  # Set to limit dataset size for quick iteration
  preprocessing_num_workers: 4

training:
  # Core training parameters
  epochs: 3
  batch_size: 16
  eval_batch_size: 32
  
  # Optimization
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler: "linear"
  
  # Evaluation and saving
  eval_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 2
  
  # Logging
  logging_steps: 50
  log_predictions: true
  
  # Early stopping (disabled by default)
  early_stopping: false
  early_stopping_patience: 3

model:
  {% if cookiecutter.modality == 'nlp' -%}
  checkpoint: "{{ cookiecutter.model_checkpoint.nlp }}"
  max_length: 512
  num_labels: 2
  dropout: 0.1
  {% elif cookiecutter.modality == 'speech' and cookiecutter.speech_task == 'asr' -%}
  checkpoint: "{{ cookiecutter.model_checkpoint.speech_asr }}"
  sample_rate: 16000
  max_audio_length: 30
  language: "en"
  task: "transcribe"
  {% elif cookiecutter.modality == 'speech' and cookiecutter.speech_task == 'tts' -%}
  checkpoint: "{{ cookiecutter.model_checkpoint.speech_tts }}"
  sample_rate: 24000
  max_tokens: 1024
  temperature: 0.8
  voice_preset: "default"
  {% elif cookiecutter.modality == 'vision' -%}
  checkpoint: "{{ cookiecutter.model_checkpoint.vision }}"
  image_size: 224
  num_labels: 10
  patch_size: 16
  dropout: 0.1
  {% endif -%}

inference:
  # Model serving
  host: "0.0.0.0"
  port: 8000
  workers: 1
  batch_timeout: 0.1
  max_batch_size: 8
  
  # Inference parameters
  temperature: 1.0
  do_sample: false
  
  # Optimization
  torch_compile: false
  quantization: null

compute:
  # Device settings
  device: "auto"
  fp16: true
  tf32: true
  
  # Memory optimization
  gradient_checkpointing: true
  dataloader_pin_memory: true
  
  {% if cookiecutter.cloud_provider != 'none' -%}
  # Cloud compute
  cloud_provider: "{{ cookiecutter.cloud_provider }}"
  instance_type: "g5.xlarge"
  region: "us-west-2"
  spot_instances: true
  {% endif -%}